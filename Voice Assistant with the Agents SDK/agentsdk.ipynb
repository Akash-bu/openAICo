{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, function_tool, WebSearchTool, FileSearchTool\n",
    "from agents.extensions.handoff_prompt import prompt_with_handoff_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_agent = Agent(\n",
    "    name = \"SearchAgent\",\n",
    "    instructions = (\n",
    "        \"You immediately provide an input to the WebSearchTool to find up-to-date information on the user's query.\"\n",
    "    ),\n",
    "    tools = [WebSearchTool()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created:  {'id': 'vs_67e6e378b078819183608dea190ba442', 'name': \"Elly's beauty Product Knowledge Base\", 'created_at': 1743184760, 'file_count': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'file': 'acme_product_catalogue.pdf', 'status': 'success'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def upload_file(file_path: str, vector_store_id: str):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    try:\n",
    "        file_response = client.files.create(file = open(file_path, 'rb'), purpose = \"assistants\")\n",
    "        attach_response = client.vector_stores.files.create(\n",
    "            vector_store_id = vector_store_id,\n",
    "            file_id = file_response.id\n",
    "        )\n",
    "        return {\"file\": file_name, \"status\": \"success\"}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {file_name}: {str(e)}\")\n",
    "        return {\"file\": file_name, \"status\":\"failed\", \"error\":str(e)}\n",
    "\n",
    "#create a vector store which returns a dictionary\n",
    "\n",
    "def create_vector_store(store_name: str) -> dict:\n",
    "    try:\n",
    "        vector_store = client.vector_stores.create(name = store_name)\n",
    "        details = {\n",
    "            \"id\":vector_store.id,\n",
    "            \"name\":vector_store.name,\n",
    "            \"created_at\":vector_store.created_at,\n",
    "            \"file_count\":vector_store.file_counts.completed\n",
    "        }\n",
    "        print(\"Vector store created: \", details)\n",
    "        return details\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vector store: {e}\")\n",
    "        return {}\n",
    "    \n",
    "vector_store_id = create_vector_store(\"Elly's beauty Product Knowledge Base\")\n",
    "upload_file(\"voice_agents_knowledge/acme_product_catalogue.pdf\", vector_store_id[\"id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_agent = Agent(\n",
    "    name = \"KnowledgeAgent\",\n",
    "    instructions= (\n",
    "        \"You answer user questions on our product portfolio with concise, helpful responses using the FileSearchtool.\"   \n",
    "    ),\n",
    "\n",
    "    tools = [FileSearchTool(\n",
    "        max_num_results = 3,\n",
    "        vector_store_ids= [\"vs_67e6e378b078819183608dea190ba442\"],\n",
    "    ),],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tool 1: Fetch account information (dummy) ---\n",
    "\n",
    "@function_tool\n",
    "def get_account_info(user_id: str) -> dict:\n",
    "    \"\"\"Returns dummy account information for a given user.\"\"\"\n",
    "    return{\n",
    "        \"user_id\": user_id,\n",
    "        \"name\": \"Markus Harry\",\n",
    "        \"account_balance\": \"$72.50\",\n",
    "        \"membership_status\": \"Gold Executive\"\n",
    "    }\n",
    "\n",
    "# --- Agent: Account Agent ---\n",
    "\n",
    "account_agent = Agent(\n",
    "    name=\"AccountAgent\",\n",
    "    instructions = (\n",
    "        \"You provide account information based on a user ID using the get_account_info tool.\"\n",
    "    ),\n",
    "    tools = [get_account_info],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Agent: Triage Agent ---\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name = \"Assistant\",\n",
    "    instructions = prompt_with_handoff_instructions(\"\"\"\n",
    "\n",
    "You are the virtual assistant for Elly's Shop. Welcome the user and ask how you can help.\n",
    "Based on the user's intent, route to:\n",
    "- AccountAgent for account-related queries\n",
    "- KnowledgeAgent for product FAQs\n",
    "- SearchAgent for anything requiring real-time web search\n",
    "                                                    \n",
    "\"\"\"),\n",
    "handoffs = [account_agent, knowledge_agent, search_agent],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What's my Elly's account balance doc? My user ID is 9856215499\n",
      "Your account balance is $72.50, Markus Harry. You're also a Gold Executive member.\n",
      "---\n",
      "User: Ooh i've got money to spend! How big is the input and how fast is the output of the dynamite dispenser?\n",
      "The Automated Dynamite Dispenser has a capacity of 10 sticks and dispenses them at a speed of 1 stick every 2 seconds.\n",
      "---\n",
      "User: Hmmm, what about duck hunting gear - what's trending right now?\n",
      "Staying updated with the latest trends in duck hunting gear can significantly enhance your hunting experience. Here are some of the top trending items for the 2025 season:\n",
      "\n",
      "\n",
      "\n",
      "**Banded Aspire Catalyst Waders**  \n",
      "These all-season waders feature waterproof-breathable technology, minimal-stitch construction for enhanced mobility, and PrimaLoft Aerogel insulation inserts for thermal protection. ([blog.gritroutdoors.com](https://blog.gritroutdoors.com/must-have-duck-hunting-gear-for-a-winning-season/?utm_source=openai))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**Sitka Delta Zip Wader**  \n",
      "Known for reinforced shins and knees with rugged foam pads, these waders are built with GORE-TEX material to keep you dry season after season. ([blog.gritroutdoors.com](https://blog.gritroutdoors.com/must-have-duck-hunting-gear-for-a-winning-season/?utm_source=openai))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**MOmarsh InvisiMan Blind**  \n",
      "A low-profile, one-person blind that's sturdy, durable, and easy to set up, making it ideal for solo hunters. ([bornhunting.com](https://bornhunting.com/top-duck-hunting-gear/?utm_source=openai))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**Slayer Calls Ranger Duck Call**  \n",
      "A double reed call known for its crisp and loud sound, effective in harsh weather conditions and at extreme distances. ([bornhunting.com](https://bornhunting.com/top-duck-hunting-gear/?utm_source=openai))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**B&P Dual Steel Magnum Ammunition**  \n",
      "This load features #2 steel stacked atop #3 zinc-plated steel, offering impressive performance at long ranges and in high winds. ([bornhunting.com](https://bornhunting.com/top-duck-hunting-gear/?utm_source=openai))\n",
      "\n",
      "\n",
      "Incorporating these trending items into your gear can enhance your comfort, effectiveness, and overall success in the field. \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from agents import Runner, trace \n",
    "\n",
    "async def test_queries():\n",
    "    examples = [\n",
    "        \"What's my Elly's account balance doc? My user ID is 9856215499\", # Account Agent test\n",
    "        \"Ooh i've got money to spend! How big is the input and how fast is the output of the dynamite dispenser?\", # Knowledge Agent test\n",
    "        \"Hmmm, what about duck hunting gear - what's trending right now?\", # Search Agent test\n",
    "    ]\n",
    "\n",
    "    with trace(\"Elly's App Assistant\"):\n",
    "        for query in examples:\n",
    "            result = await Runner.run(triage_agent, query)\n",
    "            print(f\"User: {query}\")\n",
    "            print(result.final_output)\n",
    "            print(\"---\")\n",
    "            \n",
    "await test_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sounddevice as sd \n",
    "from agents.voice import AudioInput, SingleAgentVoiceWorkflow, VoicePipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "Assistent is responding...\n",
      "---\n",
      "Listening...\n",
      "Assistent is responding...\n",
      "---\n",
      "Listening...\n",
      "Assistent is responding...\n",
      "---\n",
      "Listening...\n",
      "Assistent is responding...\n",
      "---\n",
      "Listening...\n",
      "Assistent is responding...\n",
      "---\n",
      "Listening...\n",
      "Assistent is responding...\n",
      "---\n",
      "Listening...\n",
      "Assistent is responding...\n",
      "---\n",
      "Listening...\n",
      "Assistent is responding...\n",
      "---\n",
      "Listening...\n",
      "Assistent is responding...\n",
      "---\n",
      "Listening...\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m         sd.wait()\n\u001b[32m     36\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m voice_assistant()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mvoice_assistant\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     20\u001b[39m audio_input = AudioInput(buffer = recording)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trace(\u001b[33m\"\u001b[39m\u001b[33mVoice Assistant\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m pipeline.run(audio_input)\n\u001b[32m     26\u001b[39m response_chunks = []\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m result.stream():\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\openAICo\\Voice Assistant with the Agents SDK\\venv\\Lib\\site-packages\\agents\\voice\\pipeline.py:61\u001b[39m, in \u001b[36mVoicePipeline.run\u001b[39m\u001b[34m(self, audio_input)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run the voice pipeline.\u001b[39;00m\n\u001b[32m     50\u001b[39m \n\u001b[32m     51\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     58\u001b[39m \u001b[33;03m    play them out.\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(audio_input, AudioInput):\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_single_turn(audio_input)\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(audio_input, StreamedAudioInput):\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_multi_turn(audio_input)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\openAICo\\Voice Assistant with the Agents SDK\\venv\\Lib\\site-packages\\agents\\voice\\pipeline.py:96\u001b[39m, in \u001b[36mVoicePipeline._run_single_turn\u001b[39m\u001b[34m(self, audio_input)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_single_turn\u001b[39m(\u001b[38;5;28mself\u001b[39m, audio_input: AudioInput) -> StreamedAudioResult:\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# Since this is single turn, we can use the TraceCtxManager to manage starting/ending the\u001b[39;00m\n\u001b[32m     88\u001b[39m     \u001b[38;5;66;03m# trace\u001b[39;00m\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m TraceCtxManager(\n\u001b[32m     90\u001b[39m         workflow_name=\u001b[38;5;28mself\u001b[39m.config.workflow_name \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mVoice Agent\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     91\u001b[39m         trace_id=\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# Automatically generated\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     94\u001b[39m         disabled=\u001b[38;5;28mself\u001b[39m.config.tracing_disabled,\n\u001b[32m     95\u001b[39m     ):\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m         input_text = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_audio_input(audio_input)\n\u001b[32m     98\u001b[39m         output = StreamedAudioResult(\n\u001b[32m     99\u001b[39m             \u001b[38;5;28mself\u001b[39m._get_tts_model(), \u001b[38;5;28mself\u001b[39m.config.tts_settings, \u001b[38;5;28mself\u001b[39m.config\n\u001b[32m    100\u001b[39m         )\n\u001b[32m    102\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstream_events\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\openAICo\\Voice Assistant with the Agents SDK\\venv\\Lib\\site-packages\\agents\\voice\\pipeline.py:79\u001b[39m, in \u001b[36mVoicePipeline._process_audio_input\u001b[39m\u001b[34m(self, audio_input)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_process_audio_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, audio_input: AudioInput) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     78\u001b[39m     model = \u001b[38;5;28mself\u001b[39m._get_stt_model()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m model.transcribe(\n\u001b[32m     80\u001b[39m         audio_input,\n\u001b[32m     81\u001b[39m         \u001b[38;5;28mself\u001b[39m.config.stt_settings,\n\u001b[32m     82\u001b[39m         \u001b[38;5;28mself\u001b[39m.config.trace_include_sensitive_data,\n\u001b[32m     83\u001b[39m         \u001b[38;5;28mself\u001b[39m.config.trace_include_sensitive_audio_data,\n\u001b[32m     84\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\openAICo\\Voice Assistant with the Agents SDK\\venv\\Lib\\site-packages\\agents\\voice\\models\\openai_stt.py:416\u001b[39m, in \u001b[36mOpenAISTTModel.transcribe\u001b[39m\u001b[34m(self, input, settings, trace_include_sensitive_data, trace_include_sensitive_audio_data)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m transcription_span(\n\u001b[32m    406\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    407\u001b[39m     \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m.to_base64() \u001b[38;5;28;01mif\u001b[39;00m trace_include_sensitive_audio_data \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    413\u001b[39m     },\n\u001b[32m    414\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m span:\n\u001b[32m    415\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.audio.transcriptions.create(\n\u001b[32m    417\u001b[39m             model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    418\u001b[39m             file=\u001b[38;5;28minput\u001b[39m.to_audio_file(),\n\u001b[32m    419\u001b[39m             prompt=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(settings.prompt),\n\u001b[32m    420\u001b[39m             language=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(settings.language),\n\u001b[32m    421\u001b[39m             temperature=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(settings.temperature),\n\u001b[32m    422\u001b[39m         )\n\u001b[32m    423\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m trace_include_sensitive_data:\n\u001b[32m    424\u001b[39m             span.span_data.output = response.text\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\openAICo\\Voice Assistant with the Agents SDK\\venv\\Lib\\site-packages\\openai\\resources\\audio\\transcriptions.py:617\u001b[39m, in \u001b[36mAsyncTranscriptions.create\u001b[39m\u001b[34m(self, file, model, include, language, prompt, response_format, stream, temperature, timestamp_granularities, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    613\u001b[39m \u001b[38;5;66;03m# It should be noted that the actual Content-Type header that will be\u001b[39;00m\n\u001b[32m    614\u001b[39m \u001b[38;5;66;03m# sent to the server will contain a `boundary` parameter, e.g.\u001b[39;00m\n\u001b[32m    615\u001b[39m \u001b[38;5;66;03m# multipart/form-data; boundary=---abc--\u001b[39;00m\n\u001b[32m    616\u001b[39m extra_headers = {\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmultipart/form-data\u001b[39m\u001b[33m\"\u001b[39m, **(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[32m--> \u001b[39m\u001b[32m617\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m    618\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/audio/transcriptions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    619\u001b[39m     body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(body, transcription_create_params.TranscriptionCreateParams),\n\u001b[32m    620\u001b[39m     files=files,\n\u001b[32m    621\u001b[39m     options=make_request_options(\n\u001b[32m    622\u001b[39m         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m    623\u001b[39m     ),\n\u001b[32m    624\u001b[39m     cast_to=_get_response_format_type(response_format),\n\u001b[32m    625\u001b[39m     stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    626\u001b[39m     stream_cls=AsyncStream[TranscriptionStreamEvent],\n\u001b[32m    627\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\openAICo\\Voice Assistant with the Agents SDK\\venv\\Lib\\site-packages\\openai\\_base_client.py:1767\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1753\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1754\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1755\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1762\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1763\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1764\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1765\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1766\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1767\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\openAICo\\Voice Assistant with the Agents SDK\\venv\\Lib\\site-packages\\openai\\_base_client.py:1461\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[39m\n\u001b[32m   1458\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1459\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1461\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m   1462\u001b[39m     cast_to=cast_to,\n\u001b[32m   1463\u001b[39m     options=options,\n\u001b[32m   1464\u001b[39m     stream=stream,\n\u001b[32m   1465\u001b[39m     stream_cls=stream_cls,\n\u001b[32m   1466\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1467\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\openAICo\\Voice Assistant with the Agents SDK\\venv\\Lib\\site-packages\\openai\\_base_client.py:1481\u001b[39m, in \u001b[36mAsyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1469\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_request\u001b[39m(\n\u001b[32m   1470\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1471\u001b[39m     cast_to: Type[ResponseT],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1476\u001b[39m     retries_taken: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   1477\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._platform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1479\u001b[39m         \u001b[38;5;66;03m# `get_platform` can make blocking IO calls so we\u001b[39;00m\n\u001b[32m   1480\u001b[39m         \u001b[38;5;66;03m# execute it earlier while we are in an async context\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1481\u001b[39m         \u001b[38;5;28mself\u001b[39m._platform = \u001b[38;5;28;01mawait\u001b[39;00m asyncify(get_platform)()\n\u001b[32m   1483\u001b[39m     \u001b[38;5;66;03m# create a copy of the options we were given so that if the\u001b[39;00m\n\u001b[32m   1484\u001b[39m     \u001b[38;5;66;03m# options are mutated later & we then retry, the retries are\u001b[39;00m\n\u001b[32m   1485\u001b[39m     \u001b[38;5;66;03m# given the original options\u001b[39;00m\n\u001b[32m   1486\u001b[39m     input_options = model_copy(options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\openAICo\\Voice Assistant with the Agents SDK\\venv\\Lib\\site-packages\\openai\\_utils\\_sync.py:84\u001b[39m, in \u001b[36masyncify.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: T_ParamSpec.args, **kwargs: T_ParamSpec.kwargs) -> T_Retval:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m to_thread(function, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\openAICo\\Voice Assistant with the Agents SDK\\venv\\Lib\\site-packages\\openai\\_utils\\_sync.py:45\u001b[39m, in \u001b[36mto_thread\u001b[39m\u001b[34m(func, *args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto_thread\u001b[39m(\n\u001b[32m     42\u001b[39m     func: Callable[T_ParamSpec, T_Retval], /, *args: T_ParamSpec.args, **kwargs: T_ParamSpec.kwargs\n\u001b[32m     43\u001b[39m ) -> T_Retval:\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sniffio.current_async_library() == \u001b[33m\"\u001b[39m\u001b[33masyncio\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _asyncio_to_thread(func, *args, **kwargs)\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m anyio.to_thread.run_sync(\n\u001b[32m     48\u001b[39m         functools.partial(func, *args, **kwargs),\n\u001b[32m     49\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\openAICo\\Voice Assistant with the Agents SDK\\venv\\Lib\\asyncio\\threads.py:25\u001b[39m, in \u001b[36mto_thread\u001b[39m\u001b[34m(func, *args, **kwargs)\u001b[39m\n\u001b[32m     23\u001b[39m ctx = contextvars.copy_context()\n\u001b[32m     24\u001b[39m func_call = functools.partial(ctx.run, func, *args, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.run_in_executor(\u001b[38;5;28;01mNone\u001b[39;00m, func_call)\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "async def voice_assistant():\n",
    "    samplerate = sd.query_devices(kind = 'input')['default_samplerate']\n",
    "\n",
    "    while True:\n",
    "        pipeline = VoicePipeline(workflow = SingleAgentVoiceWorkflow(triage_agent))\n",
    "\n",
    "        cmd = input(\"Press Enter to start recording... or esc to exit\")\n",
    "        if cmd.lower() == \"esc\":\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "\n",
    "        print(\"Listening...\")\n",
    "        recorded_chunks = []\n",
    "\n",
    "        with sd.InputStream(samplerate = samplerate, channels = 1, dtype = 'int16', callback = lambda indata, frames, time, status: recorded_chunks.append(indata.copy()) ):\n",
    "            input()\n",
    "\n",
    "        recording = np.concatenate(recorded_chunks, axis = 0)\n",
    "\n",
    "        audio_input = AudioInput(buffer = recording)\n",
    "\n",
    "        with trace(\"Voice Assistant\"):\n",
    "            result = await pipeline.run(audio_input)\n",
    "\n",
    "\n",
    "        response_chunks = []\n",
    "        async for event in result.stream():\n",
    "            if event.type == \"voice_stream_event_audio\":\n",
    "                response_chunks.append(event.data)\n",
    "\n",
    "        response_audio = np.concatenate(response_chunks, axis = 0)\n",
    "\n",
    "        print(\"Assistent is responding...\")\n",
    "        sd.play(response_audio, samplerate = samplerate)\n",
    "        sd.wait()\n",
    "        print(\"---\")\n",
    "\n",
    "await voice_assistant()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
